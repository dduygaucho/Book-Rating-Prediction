{"cells":[{"cell_type":"code","source":["from google.colab import drive \n","import os\n","drive.mount('/content/drive')\n","path = \"/content/drive/My Drive/Bookpred/Assignment 2\"\n","%cd {path}\n","!ls"],"metadata":{"id":"WHW75AUzkaQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684395557035,"user_tz":-600,"elapsed":4041,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"aa9fbb29-e583-45ed-cc49-b5194d79becb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Bookpred/Assignment 2\n"," 8.prep_IMDB.ipynb\n","'Assignment 2 Track.gdoc'\n"," bert\n"," best_model\n"," book_rating_test.csv\n"," book_rating_train.csv\n"," book_text_features_countvec\n"," book_text_features_doc2vec\n"," book_text_features_tfidf\n"," COMP30027_Project2_spec_2023S1.pdf\n","'Copy of model.png'\n"," English\n","'English (1)'\n","'Feature engineering.ipynb'\n"," full_dataset\n"," history.pkl\n"," history_v.pkl\n"," lgbm_class_weight.joblib\n"," lgbm_normal.joblib\n"," lgbm_oversampling.joblib\n"," matrix.npz\n"," Model.ipynb\n"," model.png\n"," multilingual_best\n"," multilingual_epochs\n"," output.csv\n"," Oversampling\n"," Oversampling_real\n"," partition\n"," random_forest_doc.joblib\n"," random_forest.joblib\n"," random_forest_num.joblib\n"," small_bert\n"," smallbert_class_weight.png\n"," smallbert_focal.png\n"," smallbert_normal.png\n"," smallbert_oversampling_minority.png\n"," Test\n"," test_focal.npy\n"," text_features_description.ipynb\n"," train_ind.pkl\n"," Validation\n"," val_ind.pkl\n"," val_output_smallbert_class_weights.pkl\n"," val_output_smallbert_cw.pkl\n"," val_output_smallbert_focal.pkl\n"," val_output_smallbert_normal.pkl\n"," val_output_smallbert_oversampling.pkl\n"," val_prediction_smallbert_class_weights.pkl\n"," val_prediction_smallbert_cw.pkl\n"," val_prediction_smallbert_focal.pkl\n"," val_prediction_smallbert_normal.pkl\n"," val_prediction_smallbert_oversampling.pkl\n"," xgb_model_doc.pkl\n"," xgb_model.pkl\n"," X_res.npz\n"," y_pred_best_multilingual.txt\n"," y_pred_best.txt\n"," y_pred_pretrain1.txt\n"," y_pred_test_smallbert_classweight.txt\n"," y_pred_test_smallbert.txt\n"," y_pred_val_multilingual.txt\n"," y_pred_val_smallbert_classweight.txt\n"," y_pred_val.txt\n"," y_res.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"mvhRMwh4VbE9"},"source":["# COMP30027 Machine Learning Project 2\n","\n","## Description of text features\n","\n","This notebook describes the pre-computed text features provided for Project 2. **You do not need to recompute the features yourself for this assignment** -- this information is just for your reference. However, feel free to experiment with different text features if you are interested. If you do want to try generating your own text features, some things to keep in mind:\n","- There are many different decisions you can make throughout the feature design process, from the text preprocessing to the size of the output vectors. There's no guarantee that the defaults we chose will produce the best possible text features for this classification task, so feel free to experiment with different settings.\n","- These features must be trained using a training corpus. Generally, the training corpus should not include validation samples, but for the purposes of this assignment we have used the entire non-test set (training+validation) as the training corpus, to allow you to experiment with different validation sets. If you recompute the text features as part of your own model, you should exclude validation samples and compute them on training samples only. For example, if you do N-fold cross-validation, this means generating N sets of features for N different training-validation splits."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"mhXStFhhVbFB","executionInfo":{"status":"ok","timestamp":1684395557401,"user_tz":-600,"elapsed":372,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# read text\n","# for DEMONSTRATION PURPOSES, the entire training set will be used to train the models and also as a test set\n","x_train_original = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n","# use recipe name as an example\n","train_corpus_name = x_train_original['Name']\n","test_name = x_train_original['Name']"]},{"cell_type":"code","source":["os.mkdir(\"book_text_features_tfidf\")"],"metadata":{"id":"3iDz6IOXhMTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs(\"partition\", exist_ok = True)\n","os.makedirs(\"partition/train\", exist_ok = True)\n","os.makedirs(\"partition/val\", exist_ok = True)\n","os.makedirs(\"partition/test\", exist_ok = True)\n","\n"],"metadata":{"id":"fq8SECQO0HHg","executionInfo":{"status":"ok","timestamp":1684395562124,"user_tz":-600,"elapsed":3,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fk4N_awLVbFD"},"source":["## Count vectorizer\n","\n","A count vectorizer converts documents to vectors which represent word counts. Each column in the output represents a different word and the values indicate the number of times that word appeared in the document. The overall size of a count vector matrix can be quite large (the number of columns is the total number of different words used across all documents in a corpus), but most entries in the matrix are zero (each document contains only a few of all the possible words). Therefore, it is most efficient to represent the count vectors as a sparse matrix."]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhutYA3efw3k","executionInfo":{"status":"ok","timestamp":1684395573472,"user_tz":-600,"elapsed":11046,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"8db998dc-1e21-488b-92e9-fc1afdf8c1e0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}]},{"cell_type":"code","execution_count":19,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"hWxD1q3OVbFD","executionInfo":{"status":"ok","timestamp":1684396102924,"user_tz":-600,"elapsed":6353,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"f798ee74-dc17-45c1-ee4e-826145c373eb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Publisher\n","(23063, 3524)\n","(5766, 3524)\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","import pandas as pd\n","import re\n","import nltk\n","from string import punctuation\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from scipy.sparse import csr_matrix, save_npz, load_npz\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","\n","\n","def preprocessing_pipeline(text):\n","    # Remove HTML tags\n","  clean = re.compile('<.*?>')\n","  text = re.sub(clean, '', text)\n","  \n","  # Remove punctuation\n","  text = ''.join(c for c in text if c not in punctuation)\n","  \n","  # Tokenize text\n","  tokens = text.split()\n","  \n","  # Remove stopwords\n","  stopwords_list = stopwords.words('english')\n","  words = [word for word in tokens if word.lower() not in stopwords_list]\n","  \n","  # Stem words\n","  stemmer = PorterStemmer()\n","  stemmed_words = [stemmer.stem(word) for word in words]\n","  \n","  # Join cleaned and stemmed words back into sentence\n","  return \" \".join(stemmed_words)\n","\n","\n","def create_npz_file():\n","  train_df = pd.read_csv(r\"book_rating_train.csv\", \n","                         index_col = False, delimiter = ',', header=0)\n","  test_df = pd.read_csv(r\"book_rating_test.csv\", \n","                        index_col = False, delimiter = ',', header=0)\n","  \n","  # features = ['Authors', 'Name', 'Description']\n","  features = ['Publisher']\n","\n","\n","  for feature in features:\n","    print(feature)\n","    train_target = train_df[feature].fillna(\"\").apply(lambda x: preprocessing_pipeline(x))\n","    test_target = test_df[feature].fillna(\"\").apply(lambda x: preprocessing_pipeline(x))\n","    # preprocess text and compute counts\n","    vectorizer = TfidfVectorizer(stop_words='english').fit(train_target)\n","        \n","    # generate counts for a new set of documents\n","    vec_train = vectorizer.transform(train_target)\n","    vec_test = vectorizer.transform(test_target)\n","    print(vec_train.shape)\n","    print(vec_test.shape)\n","\n","\n","\n","\n","    if(feature == 'Description'):\n","      save_npz(f'./book_text_features_tfidf/train_desc_vec.npz', vec_train)\n","      save_npz(f'./book_text_features_tfidf/test_desc_vec.npz', vec_test)\n","\n","    else:\n","      save_npz(f'./book_text_features_tfidf/train_{feature.lower()}_vec.npz', vec_train)\n","      save_npz(f'./book_text_features_tfidf/test_{feature.lower()}_vec.npz', vec_test)\n","\n","create_npz_file()"]},{"cell_type":"code","source":["train_df = pd.read_csv(r\"book_rating_train.csv\", \n","                        index_col = False, delimiter = ',', header=0)\n","train_df['Publisher'].fillna(\"\").apply(lambda x: preprocessing_pipeline(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kK-NG7qbzsti","executionInfo":{"status":"ok","timestamp":1684396018104,"user_tz":-600,"elapsed":6784,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"063db15c-f760-4456-a2ee-e55dc1d7c045"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                     teach resourc\n","1                         doubleday\n","2                     chronicl book\n","3                        bison book\n","4                  penguin book ltd\n","                    ...            \n","23058                         21361\n","23059         hmh book young reader\n","23060    rowman littlefield publish\n","23061            new amsterdam book\n","23062                        puffin\n","Name: Publisher, Length: 23063, dtype: object"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["import re\n","seed = 42\n","def partition():\n","  df = pd.read_csv(r\"book_rating_train.csv\", \n","                         index_col = False, delimiter = ',', header=0)\n","  train = pd.DataFrame()\n","  val = pd.DataFrame()\n","  for value in pd.unique(df['rating_label']):\n","    class_df = df[df['rating_label'] == value]\n","    # print(class_df.shape)\n","    class_df_val = class_df.sample(int(0.2*len(class_df)), replace = False, random_state = seed)\n","    # print(class_df_val.shape)\n","    train_class_df = class_df.drop(class_df_val.index)\n","    # print(train_class_df.shape)\n","    train = pd.concat([train, train_class_df])\n","    val = pd.concat([val, class_df_val])\n","\n","  train = train.sample(frac = 1, random_state = seed)\n","  val = val.sample(frac = 1, random_state = seed)\n","\n","\n","  return train, val\n","\n","\n","train, val = partition()\n","print(train.shape, val.shape)\n"],"metadata":{"id":"xXwuEgjy0rXS","executionInfo":{"status":"ok","timestamp":1684328773804,"user_tz":-600,"elapsed":1999,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"04e163d5-1b6d-45fb-e91d-9a9bb73d04ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(18452, 10) (4611, 10)\n"]}]},{"cell_type":"code","source":["test_df = pd.read_csv(r\"book_rating_test.csv\", \n","                      index_col = False, delimiter = ',', header=0)\n","test_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_bPR5-O0cGD","executionInfo":{"status":"ok","timestamp":1684328915802,"user_tz":-600,"elapsed":1737,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"756cedf6-724d-4aa1-8ae8-fdda90ff932b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5766, 9)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["print(len(train.index))\n","\n","print(len(val.index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXgINiPLAZD_","executionInfo":{"status":"ok","timestamp":1683963478148,"user_tz":-600,"elapsed":7,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"e1d23d54-56e0-40fe-af2e-29909ea3e5bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["18452\n","4611\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","# create a list of strings\n","\n","# save the list to a pickle file\n","with open('train_ind.pkl', 'wb') as f:\n","    pickle.dump(list(train.index), f)\n","\n","\n","with open('val_ind.pkl', 'wb') as f:\n","    pickle.dump(list(val.index), f)"],"metadata":{"id":"5jq5c026AsP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_npz_file(train, val, test):\n","  # os.makedirs()\n","  features = ['Authors', 'Name', 'Description']\n","\n","  for feature in features:\n","    print(feature)\n","    train_target = train[feature].apply(lambda x: preprocessing_pipeline(x))\n","    val_target = val[feature].apply(lambda x: preprocessing_pipeline(x))\n","    test_target = test[feature].apply(lambda x: preprocessing_pipeline(x))\n","    # preprocess text and compute counts\n","    vectorizer = TfidfVectorizer(stop_words='english').fit(train_target)\n","        \n","    # generate counts for a new set of documents\n","    # vec_train = vectorizer.transform(train_target)\n","    # vec_val = vectorizer.transform(val_target)\n","    # print(vec_train.shape)\n","    # print(vec_val.shape)\n","    vec_test = vectorizer.transform(test_target)\n","    print(vec_test.shape)\n","\n","    if(feature == 'Description'):\n","      # save_npz(f'./partition/train/train_desc_vec.npz', vec_train)\n","      # save_npz(f'./partition/val/val_desc_vec.npz', vec_val)\n","      save_npz(f'./partition/test/test_desc_vec.npz', vec_test)\n","\n","\n","    else:\n","      # save_npz(f'./partition/train/train_{feature.lower()}_vec.npz', vec_train)\n","      # save_npz(f'./partition/val/val_{feature.lower()}_vec.npz', vec_val)\n","      save_npz(f'./partition/test/test_{feature.lower()}_vec.npz', vec_test)\n","\n","\n","partition_npz_file(train, val, test_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srblBi5K2yBJ","executionInfo":{"status":"ok","timestamp":1684329335550,"user_tz":-600,"elapsed":87233,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"a8344522-7769-4118-d687-ff6bc2b65bc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Authors\n","(5766, 11342)\n","Name\n","(5766, 15296)\n","Description\n","(5766, 90674)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BlWz2xUHVbFF"},"source":["## doc2vec\n","\n","doc2vec methods are an extension of word2vec. word2vec maps words to a high-dimensional vector space in such a way that words which appear in similar contexts will be close together in the space. doc2vec does a similar embedding for multi-word passages. The doc2vec (or Paragraph Vector) method was introduced by:\n","\n","**Le & Mikolov (2014)** Distributed Representations of Sentences and Documents<br>\n","https://arxiv.org/pdf/1405.4053v2.pdf\n","\n","The implementation of doc2vec used for this project is from gensim and documented here:<br>\n","https://radimrehurek.com/gensim/models/doc2vec.html\n","\n","The size of the output vector is a free parameter. Most implemementations use around 100-300 dimensions, but the best size depends on the problem you're trying to solve with the embeddings and the number of training samples, so you may wish to try different vector sizes. We provided doc2vec features for Name (vec_size = 100), Authors (vec_size = 20) and Description (vec_size = 100). The vectors themselves represent directions in a high-dimensional concept space; the columns do not represent specific words or phrases. Values in the vector are continuous real numbers and can be negative."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBFRVNkCVbFF","outputId":"7f144ffc-0190-45f5-858d-d086727ef91b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(23063, 100)\n"]}],"source":["import gensim\n","\n","# size of the output vector\n","vec_size = 100\n","\n","# function to preprocess and tokenize text\n","def tokenize_corpus(txt, tokens_only=False):\n","    for i, line in enumerate(txt):\n","        tokens = gensim.utils.simple_preprocess(line)\n","        if tokens_only:\n","            yield tokens\n","        else:\n","            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n","\n","# tokenize a training corpus\n","corpus_name = list(tokenize_corpus(train_corpus_name))\n","\n","# train doc2vec on the training corpus\n","model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n","model.build_vocab(corpus_name)\n","model.train(corpus_name, total_examples=model.corpus_count, epochs=model.epochs)\n","\n","# tokenize new documents\n","doc = list(tokenize_corpus(test_name, tokens_only=True))\n","\n","# generate embeddings for the new documents\n","x_test_name = np.zeros((len(doc),vec_size))\n","for i in range(len(doc)):\n","    x_test_name[i,:] = model.infer_vector(doc[i])\n","    \n","# check the shape of doc_emb\n","print(x_test_name.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHeFWAd6VbFG"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Text preparation"],"metadata":{"id":"U4GD_TNoji7s"}},{"cell_type":"code","source":["import os \n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"6MkphS0njlUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"book_rating_train.csv\")\n","train_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hrJIYV6ckd2L","executionInfo":{"status":"ok","timestamp":1683619982089,"user_tz":-600,"elapsed":1198,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"4081fadb-d09e-4bca-e9f1-fb84c83fa8e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    Name  \\\n","0                     Best of Dr Jean: Reading & Writing   \n","1                                    Here All Dwell Free   \n","2                                  Boomer's Big Surprise   \n","3      I'll Go and Do More: Annie Dodge Wauneka, Nava...   \n","4                                                     Us   \n","...                                                  ...   \n","23058                                 Black Coffee Blues   \n","23059        America's Champion Swimmer: Gertrude Ederle   \n","23060                 Crime and Custom in Savage Society   \n","23061  The Name and Nature of Poetry and Other Select...   \n","23062                            Redemption (Sevens, #7)   \n","\n","                      Authors  PublishYear  PublishMonth  PublishDay  \\\n","0             Jean R. Feldman         2005             6           1   \n","1      Gertrud Mueller Nelson         1991            10           1   \n","2       Constance W. McGeorge         2005             3          31   \n","3          Carolyn Niethammer         2004             9           1   \n","4         Richard       Mason         2005             7           7   \n","...                       ...          ...           ...         ...   \n","23058           Henry Rollins         1997             8           1   \n","23059          David A. Adler         2005             6           1   \n","23060    Bronisław Malinowski         1989             2          15   \n","23061            A.E. Housman         1998             4          21   \n","23062           Scott Wallens         2002             7           8   \n","\n","                             Publisher Language  pagesNumber  \\\n","0                   Teaching Resources      NaN           48   \n","1                            DoubleDay      NaN          364   \n","2                      Chronicle Books      NaN           32   \n","3                          Bison Books      NaN          293   \n","4                    Penguin Books Ltd      eng          352   \n","...                                ...      ...          ...   \n","23058                          2.13.61      eng          120   \n","23059      HMH Books for Young Readers      NaN           32   \n","23060  Rowman & Littlefield Publishers      NaN          132   \n","23061              New Amsterdam Books      NaN          136   \n","23062                           Puffin      NaN          192   \n","\n","                                             Description  rating_label  \n","0      Teachers will turn to this treasury of ideas a...           4.0  \n","1      Every human being lives a fairy tale -- an unc...           4.0  \n","2      <i>Boomer's Big Surprise</i> will have special...           4.0  \n","3      <i>I'll Go and Do More</i> is the story of Ann...           4.0  \n","4      Since their days at Oxford, they've gone their...           3.0  \n","...                                                  ...           ...  \n","23058  \"If I lose the light of the sun, I will write ...           4.0  \n","23059  Trudy Ederle loved to swim, and she was determ...           4.0  \n","23060  Bronislaw Malinowski achieved international re...           4.0  \n","23061  Lovers of Housman's poetry and admirers of his...           4.0  \n","23062  Before the accident, Peter spent every weekend...           4.0  \n","\n","[23063 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-35ede04e-ab73-43ea-81db-899208c1be2d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Authors</th>\n","      <th>PublishYear</th>\n","      <th>PublishMonth</th>\n","      <th>PublishDay</th>\n","      <th>Publisher</th>\n","      <th>Language</th>\n","      <th>pagesNumber</th>\n","      <th>Description</th>\n","      <th>rating_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Best of Dr Jean: Reading &amp; Writing</td>\n","      <td>Jean R. Feldman</td>\n","      <td>2005</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>Teaching Resources</td>\n","      <td>NaN</td>\n","      <td>48</td>\n","      <td>Teachers will turn to this treasury of ideas a...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Here All Dwell Free</td>\n","      <td>Gertrud Mueller Nelson</td>\n","      <td>1991</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>DoubleDay</td>\n","      <td>NaN</td>\n","      <td>364</td>\n","      <td>Every human being lives a fairy tale -- an unc...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Boomer's Big Surprise</td>\n","      <td>Constance W. McGeorge</td>\n","      <td>2005</td>\n","      <td>3</td>\n","      <td>31</td>\n","      <td>Chronicle Books</td>\n","      <td>NaN</td>\n","      <td>32</td>\n","      <td>&lt;i&gt;Boomer's Big Surprise&lt;/i&gt; will have special...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I'll Go and Do More: Annie Dodge Wauneka, Nava...</td>\n","      <td>Carolyn Niethammer</td>\n","      <td>2004</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>Bison Books</td>\n","      <td>NaN</td>\n","      <td>293</td>\n","      <td>&lt;i&gt;I'll Go and Do More&lt;/i&gt; is the story of Ann...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Us</td>\n","      <td>Richard       Mason</td>\n","      <td>2005</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>Penguin Books Ltd</td>\n","      <td>eng</td>\n","      <td>352</td>\n","      <td>Since their days at Oxford, they've gone their...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>23058</th>\n","      <td>Black Coffee Blues</td>\n","      <td>Henry Rollins</td>\n","      <td>1997</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>2.13.61</td>\n","      <td>eng</td>\n","      <td>120</td>\n","      <td>\"If I lose the light of the sun, I will write ...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>23059</th>\n","      <td>America's Champion Swimmer: Gertrude Ederle</td>\n","      <td>David A. Adler</td>\n","      <td>2005</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>HMH Books for Young Readers</td>\n","      <td>NaN</td>\n","      <td>32</td>\n","      <td>Trudy Ederle loved to swim, and she was determ...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>23060</th>\n","      <td>Crime and Custom in Savage Society</td>\n","      <td>Bronisław Malinowski</td>\n","      <td>1989</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>Rowman &amp; Littlefield Publishers</td>\n","      <td>NaN</td>\n","      <td>132</td>\n","      <td>Bronislaw Malinowski achieved international re...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>23061</th>\n","      <td>The Name and Nature of Poetry and Other Select...</td>\n","      <td>A.E. Housman</td>\n","      <td>1998</td>\n","      <td>4</td>\n","      <td>21</td>\n","      <td>New Amsterdam Books</td>\n","      <td>NaN</td>\n","      <td>136</td>\n","      <td>Lovers of Housman's poetry and admirers of his...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>23062</th>\n","      <td>Redemption (Sevens, #7)</td>\n","      <td>Scott Wallens</td>\n","      <td>2002</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>Puffin</td>\n","      <td>NaN</td>\n","      <td>192</td>\n","      <td>Before the accident, Peter spent every weekend...</td>\n","      <td>4.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>23063 rows × 10 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35ede04e-ab73-43ea-81db-899208c1be2d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35ede04e-ab73-43ea-81db-899208c1be2d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35ede04e-ab73-43ea-81db-899208c1be2d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import re\n","def create_directory(df):\n","  os.makedirs(f\"/content/drive/MyDrive/Bookpred/Assignment 2/English\")\n","  for value in pd.unique(df['rating_label']):\n","    os.makedirs(f\"/content/drive/MyDrive/Bookpred/Assignment 2/English/{value}\")\n","    class_df = df[df['rating_label'] == value]\n","\n","    for i, row in class_df.iterrows():\n","      description = row['Description']\n","      clean = re.compile('<.*?>')\n","      description = re.sub(clean, '', description)\n","\n","      filename = f\"/content/drive/MyDrive/Bookpred/Assignment 2/English/{value}/{np.random.randint(0,1000)}_{np.random.randint(0,1000)}_{np.random.randint(0,100)}.txt\"      \n","      with open(filename, \"w\") as fp:\n","        fp.write(description)\n","    print(class_df.shape)\n","\n","create_directory(train_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJpV1Ug0oKlF","executionInfo":{"status":"ok","timestamp":1683622768525,"user_tz":-600,"elapsed":230663,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"04478caf-aa24-429d-8007-270ff9fbd461"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(16208, 10)\n","(5864, 10)\n","(991, 10)\n"]}]},{"cell_type":"code","source":["len(os.listdir(\"/content/drive/MyDrive/Bookpred/Assignment 2/English/5.0\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gbsuq9eVp7kr","executionInfo":{"status":"ok","timestamp":1683622988132,"user_tz":-600,"elapsed":312,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"68bcad65-cf1c-4031-f866-275ea19285c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["991"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["!find ./ -type f -name \"*.txt\" -delete"],"metadata":{"id":"AYq8VKS2qIpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","folder_path = '/content/drive/MyDrive/Bookpred/Assignment 2'  # current folder path\n","for file in os.listdir(folder_path):\n","    if file.endswith('.txt'):\n","        os.remove(os.path.join(folder_path, file))\n"],"metadata":{"id":"gYNypLPAr52R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","shutil.rmtree('/content/drive/MyDrive/Bookpred/Assignment 2/English', ignore_errors=True)\n"],"metadata":{"id":"ynxLojp7tJKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.listdir()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPKJf0WvtLtq","executionInfo":{"status":"ok","timestamp":1683622518385,"user_tz":-600,"elapsed":7,"user":{"displayName":"Tran Duy","userId":"01594096690661313350"}},"outputId":"f3275051-1708-40e0-a601-e0b5a62a293a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['book_rating_train.csv',\n"," 'book_rating_test.csv',\n"," 'COMP30027_Project2_spec_2023S1.pdf',\n"," 'random_forest.joblib',\n"," 'random_forest_doc.joblib',\n"," '.ipynb_checkpoints',\n"," 'book_text_features_doc2vec',\n"," 'book_text_features_countvec',\n"," 'Feature engineering.ipynb',\n"," 'xgb_model.pkl',\n"," 'xgb_model_doc.pkl',\n"," 'random_forest_num.joblib',\n"," '8.PretrainedBERT_IMDB.ipynb',\n"," 'matrix.npz',\n"," 'book_text_features_tfidf',\n"," 'text_features_description.ipynb',\n"," 'Assignment 2 Track.gdoc',\n"," 'Model.ipynb',\n"," 'output.csv']"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":[],"metadata":{"id":"g-2amKYBuNp4"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}